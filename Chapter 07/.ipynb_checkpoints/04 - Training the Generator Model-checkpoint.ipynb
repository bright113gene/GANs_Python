{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "curr_dir = os.getcwd() + '/New'\n",
    "os.mkdir(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Model Synthesizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, dis_model):\n",
    "    model = keras.models.Sequential()\n",
    "    dis_model.trainable = False\n",
    "    model.add(g_model)\n",
    "    model.add(dis_model)\n",
    "    opt = keras.optimizers.adam(learning_rate= 0.0002,\n",
    "                                beta_1= 0.5)\n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer= opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Model Synthesizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(units= 128 * 7 * 7,\n",
    "                                 input_dim= latent_dim))\n",
    "    model.add(keras.layers.Reshape((7, 7, 128)))\n",
    "    \n",
    "    model.add(keras.layers.Conv2DTranspose(filters= 128,\n",
    "                                           kernel_size= (4,4),\n",
    "                                           strides= (2,2),\n",
    "                                           padding= 'same'))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Conv2DTranspose(filters= 128,\n",
    "                                           kernel_size= (4,4),\n",
    "                                           strides= (2,2),\n",
    "                                           padding= 'same'))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Conv2D(filters= 1,\n",
    "                                  kernel_size= (7,7),\n",
    "                                  activation= 'sigmoid',\n",
    "                                  padding= 'same'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(input_shape= (28,28,1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters= 64,\n",
    "                                  strides= (2,2),\n",
    "                                  kernel_size= (3, 3),\n",
    "                                  padding= 'same',\n",
    "                                  input_shape= input_shape))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Conv2D(filters= 64,\n",
    "                                  strides= (2,2),\n",
    "                                  kernel_size= (3, 3),\n",
    "                                  padding= 'same',\n",
    "                                  input_shape= input_shape))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(units= 1,\n",
    "                                 activation= 'sigmoid'))\n",
    "    opt = keras.optimizers.adam(learning_rate= 0.0002, beta_1= 0.5)\n",
    "    model.compile(loss= 'binary_crossentropy', optimizer= opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Real Exampels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    (X_train, _), (_, _) = keras.datasets.mnist.load_data()\n",
    "    X_train = np.expand_dims(X_train, axis= -1).astype('float32') / 255.0\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Fake Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape((n_samples, latent_dim))\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    X_input = generate_latent_points(latent_dim= latent_dim,\n",
    "                               n_samples= n_samples)\n",
    "    X = g_model.predict(X_input)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model(epoch, g_model, d_model, latent_dim, dataset, n_samples= 100):\n",
    "    X_real, y_real = generate_real_samples(dataset= dataset, n_samples= n_samples)\n",
    "    X_fake, y_fake = generate_fake_samples(g_model= g_model,\n",
    "                                           latent_dim= latent_dim,\n",
    "                                           n_samples= n_samples)\n",
    "    \n",
    "    acc_real = d_model.evaluate(X_real, y_real, verbose= 0)\n",
    "    acc_fake = d_model.evaluate(X_fake, y_fake, verbose= 0)\n",
    "    print(f'Epoch: {epoch + 1}, Accuracy on real data: {acc_real}, Accuracy on generated data: {acc_fake}')\n",
    "    save_plot(X_fake, epoch= epoch, n=10)\n",
    "    model_name = f'./New/generator_model_{epoch + 1}.h5'\n",
    "    g_model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(examples, epoch, n=10):\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, 1+i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(examples[i, :, :, 0], cmap= 'gray')\n",
    "    filename = f'./New/generated_plot_epoch{epoch + 1}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan_model, g_model, d_model, dataset, latent_dim, epochs= 100, batch_size= 256):\n",
    "    half_batch = int(batch_size / 2)\n",
    "    batch_per_epoch = int(dataset.shape[0]/batch_size)\n",
    "    for i in range(epochs):\n",
    "        for j in range(batch_per_epoch):\n",
    "            # Generating real and fake examples\n",
    "            X_real, y_real = generate_real_samples(dataset= dataset, n_samples= half_batch)\n",
    "            X_fake, y_fake = generate_fake_samples(g_model= g_model,\n",
    "                                                   latent_dim= latent_dim,\n",
    "                                                   n_samples= half_batch)\n",
    "            # Stacking the training datas\n",
    "            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "            # Training the discriminator mode\n",
    "            d_loss = d_model.train_on_batch(X, y)\n",
    "            \n",
    "            # Generating image from latent space\n",
    "            x_input = generate_latent_points(latent_dim= latent_dim,\n",
    "                                             n_samples= batch_size)\n",
    "            \n",
    "            X_gan = generate_latent_points(latent_dim= latent_dim,\n",
    "                                           n_samples= batch_size)\n",
    "            \n",
    "            y_gan = np.ones((batch_size, 1))\n",
    "            \n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            print(f'Epoch: {i + 1}, batch: {j}/{batch_per_epoch},dloss: {d_loss}, gloss: {g_loss}')\n",
    "            \n",
    "        # Saving the model every once in a while\n",
    "        summarize_model(epoch= i,\n",
    "                        g_model= g_model,\n",
    "                        d_model= d_model,\n",
    "                        dataset= dataset,\n",
    "                        latent_dim= latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, batch: 0/234,dloss: 0.7055865526199341, gloss: 0.6933325529098511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, batch: 1/234,dloss: 0.6923713684082031, gloss: 0.715496838092804\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "dataset = load_mnist_data()\n",
    "g_model = define_generator(latent_dim= latent_dim)\n",
    "d_model = define_discriminator()\n",
    "gan_model = define_gan(g_model= g_model, dis_model= d_model)\n",
    "\n",
    "# Training the GAN for MNIST!!\n",
    "train_gan(gan_model= gan_model,\n",
    "          g_model= g_model,\n",
    "          d_model= d_model,\n",
    "          dataset= dataset,\n",
    "          latent_dim= latent_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
