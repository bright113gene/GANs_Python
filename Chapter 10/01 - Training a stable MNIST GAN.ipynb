{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, dis_model):\n",
    "    model = keras.models.Sequential()\n",
    "    dis_model.trainable = False\n",
    "    model.add(g_model)\n",
    "    model.add(dis_model)\n",
    "    opt = keras.optimizers.adam(learning_rate= 0.0002,\n",
    "                                beta_1= 0.5)\n",
    "    \n",
    "    model.compile(loss= 'binary_crossentropy',\n",
    "                  optimizer= opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(units= 128 * 7 * 7,\n",
    "                                 input_dim= latent_dim))\n",
    "    model.add(keras.layers.Reshape((7, 7, 128)))\n",
    "    \n",
    "    model.add(keras.layers.Conv2DTranspose(filters= 128,\n",
    "                                           kernel_size= (4,4),\n",
    "                                           strides= (2,2),\n",
    "                                           padding= 'same'))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Conv2DTranspose(filters= 128,\n",
    "                                           kernel_size= (4,4),\n",
    "                                           strides= (2,2),\n",
    "                                           padding= 'same'))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Conv2D(filters= 1,\n",
    "                                  kernel_size= (7,7),\n",
    "                                  activation= 'sigmoid',\n",
    "                                  padding= 'same'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(input_shape= (28,28,1)):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters= 64,\n",
    "                                  strides= (2,2),\n",
    "                                  kernel_size= (3, 3),\n",
    "                                  padding= 'same',\n",
    "                                  input_shape= input_shape))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Conv2D(filters= 64,\n",
    "                                  strides= (2,2),\n",
    "                                  kernel_size= (3, 3),\n",
    "                                  padding= 'same',\n",
    "                                  input_shape= input_shape))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(units= 1,\n",
    "                                 activation= 'sigmoid'))\n",
    "    opt = keras.optimizers.adam(learning_rate= 0.0002, beta_1= 0.5)\n",
    "    model.compile(loss= 'binary_crossentropy', optimizer= opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    (X_train, y_train), (_, _) = keras.datasets.mnist.load_data()\n",
    "    indices = y_train == 8\n",
    "    X_train = X_train[indices]    \n",
    "    X_train = np.expand_dims(X_train, axis= -1).astype('float32') / 255.0\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    ix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = np.random.randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape((n_samples, latent_dim))\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    X_input = generate_latent_points(latent_dim= latent_dim,\n",
    "                               n_samples= n_samples)\n",
    "    X = g_model.predict(X_input)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model(epoch, g_model, d_model, latent_dim, dataset, n_samples= 100):\n",
    "    X_real, y_real = generate_real_samples(dataset= dataset, n_samples= n_samples)\n",
    "    X_fake, y_fake = generate_fake_samples(g_model= g_model,\n",
    "                                           latent_dim= latent_dim,\n",
    "                                           n_samples= n_samples)\n",
    "    \n",
    "    acc_real = d_model.evaluate(X_real, y_real, verbose= 0)\n",
    "    acc_fake = d_model.evaluate(X_fake, y_fake, verbose= 0)\n",
    "    print(f'Epoch: {epoch + 1}, Accuracy on real data: {acc_real}, Accuracy on generated data: {acc_fake}')\n",
    "    save_plot(X_fake, epoch= epoch, n=10)\n",
    "    model_name = f'./New/generator_model_{epoch + 1}.h5'\n",
    "    g_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(examples, epoch, n=5):\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, 1+i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(examples[i, :, :, 0], cmap= 'gray')\n",
    "    filename = f'./New/generated_plot_epoch{epoch + 1}.png'\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan_model, g_model, d_model, dataset, latent_dim, epochs= 100, batch_size= 128):\n",
    "    half_batch = int(batch_size / 2)\n",
    "    batch_per_epoch = int(dataset.shape[0]/batch_size)\n",
    "    for i in range(epochs):\n",
    "        for j in range(batch_per_epoch):\n",
    "            # Generating real and fake examples\n",
    "            X_real, y_real = generate_real_samples(dataset= dataset, n_samples= half_batch)\n",
    "            X_fake, y_fake = generate_fake_samples(g_model= g_model,\n",
    "                                                   latent_dim= latent_dim,\n",
    "                                                   n_samples= half_batch)\n",
    "            # Stacking the training datas\n",
    "            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "            # Training the discriminator mode\n",
    "            d_loss = d_model.train_on_batch(X, y)\n",
    "            \n",
    "            # Generating image from latent space\n",
    "            x_input = generate_latent_points(latent_dim= latent_dim,\n",
    "                                             n_samples= batch_size)\n",
    "            \n",
    "            X_gan = generate_latent_points(latent_dim= latent_dim,\n",
    "                                           n_samples= batch_size)\n",
    "            \n",
    "            y_gan = np.ones((batch_size, 1))\n",
    "            \n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            print(f'Epoch: {i + 1}, batch: {j}/{batch_per_epoch},dloss: {d_loss}, gloss: {g_loss}')\n",
    "            \n",
    "        # Saving the model every once in a while\n",
    "        summarize_model(epoch= i,\n",
    "                        g_model= g_model,\n",
    "                        d_model= d_model,\n",
    "                        dataset= dataset,\n",
    "                        latent_dim= latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, batch: 0/45,dloss: 0.6926295757293701, gloss: 0.6922855377197266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, batch: 1/45,dloss: 0.6828494071960449, gloss: 0.7103743553161621\n",
      "Epoch: 1, batch: 2/45,dloss: 0.673771858215332, gloss: 0.7182149887084961\n",
      "Epoch: 1, batch: 3/45,dloss: 0.6695533394813538, gloss: 0.7322429418563843\n",
      "Epoch: 1, batch: 4/45,dloss: 0.6620096564292908, gloss: 0.7466903924942017\n",
      "Epoch: 1, batch: 5/45,dloss: 0.6532983183860779, gloss: 0.7609587907791138\n",
      "Epoch: 1, batch: 6/45,dloss: 0.6450750827789307, gloss: 0.7778518199920654\n",
      "Epoch: 1, batch: 7/45,dloss: 0.6403759717941284, gloss: 0.7905789613723755\n",
      "Epoch: 1, batch: 8/45,dloss: 0.6308712363243103, gloss: 0.803397536277771\n",
      "Epoch: 1, batch: 9/45,dloss: 0.6240161657333374, gloss: 0.8127363920211792\n",
      "Epoch: 1, batch: 10/45,dloss: 0.6138575673103333, gloss: 0.8215730786323547\n",
      "Epoch: 1, batch: 11/45,dloss: 0.6109062433242798, gloss: 0.8262985944747925\n",
      "Epoch: 1, batch: 12/45,dloss: 0.6023463010787964, gloss: 0.8327080607414246\n",
      "Epoch: 1, batch: 13/45,dloss: 0.6024649739265442, gloss: 0.8365345597267151\n",
      "Epoch: 1, batch: 14/45,dloss: 0.599471926689148, gloss: 0.8325133323669434\n",
      "Epoch: 1, batch: 15/45,dloss: 0.5954886674880981, gloss: 0.8218391537666321\n",
      "Epoch: 1, batch: 16/45,dloss: 0.5836058855056763, gloss: 0.8166251182556152\n",
      "Epoch: 1, batch: 17/45,dloss: 0.5881525278091431, gloss: 0.8017029762268066\n",
      "Epoch: 1, batch: 18/45,dloss: 0.5792791843414307, gloss: 0.7902116775512695\n",
      "Epoch: 1, batch: 19/45,dloss: 0.5813207626342773, gloss: 0.7764571905136108\n",
      "Epoch: 1, batch: 20/45,dloss: 0.5794620513916016, gloss: 0.7651848196983337\n",
      "Epoch: 1, batch: 21/45,dloss: 0.5733504295349121, gloss: 0.7571293115615845\n",
      "Epoch: 1, batch: 22/45,dloss: 0.5722918510437012, gloss: 0.7469184398651123\n",
      "Epoch: 1, batch: 23/45,dloss: 0.553035318851471, gloss: 0.7376667857170105\n",
      "Epoch: 1, batch: 24/45,dloss: 0.5549600720405579, gloss: 0.7342727780342102\n",
      "Epoch: 1, batch: 25/45,dloss: 0.5531668066978455, gloss: 0.7265808582305908\n",
      "Epoch: 1, batch: 26/45,dloss: 0.5361347794532776, gloss: 0.724820613861084\n",
      "Epoch: 1, batch: 27/45,dloss: 0.5320490598678589, gloss: 0.7195332050323486\n",
      "Epoch: 1, batch: 28/45,dloss: 0.5189390182495117, gloss: 0.7146698236465454\n",
      "Epoch: 1, batch: 29/45,dloss: 0.5108033418655396, gloss: 0.7129290103912354\n",
      "Epoch: 1, batch: 30/45,dloss: 0.5130521059036255, gloss: 0.7078782320022583\n",
      "Epoch: 1, batch: 31/45,dloss: 0.5001194477081299, gloss: 0.7004203796386719\n",
      "Epoch: 1, batch: 32/45,dloss: 0.4983270466327667, gloss: 0.6853702068328857\n",
      "Epoch: 1, batch: 33/45,dloss: 0.5043561458587646, gloss: 0.6651468276977539\n",
      "Epoch: 1, batch: 34/45,dloss: 0.5007309913635254, gloss: 0.647160530090332\n",
      "Epoch: 1, batch: 35/45,dloss: 0.5183205008506775, gloss: 0.6037226915359497\n",
      "Epoch: 1, batch: 36/45,dloss: 0.5754786133766174, gloss: 0.609224259853363\n",
      "Epoch: 1, batch: 37/45,dloss: 0.605087399482727, gloss: 0.567186713218689\n",
      "Epoch: 1, batch: 38/45,dloss: 0.5834643244743347, gloss: 0.578803539276123\n",
      "Epoch: 1, batch: 39/45,dloss: 0.6289889812469482, gloss: 0.5393422245979309\n",
      "Epoch: 1, batch: 40/45,dloss: 0.6027973890304565, gloss: 0.5184303522109985\n",
      "Epoch: 1, batch: 41/45,dloss: 0.6034624576568604, gloss: 0.49503958225250244\n",
      "Epoch: 1, batch: 42/45,dloss: 0.605591893196106, gloss: 0.5463048219680786\n",
      "Epoch: 1, batch: 43/45,dloss: 0.63174968957901, gloss: 0.497020423412323\n",
      "Epoch: 1, batch: 44/45,dloss: 0.6575850248336792, gloss: 0.5657527446746826\n",
      "Epoch: 1, Accuracy on real data: 0.33178924798965453, Accuracy on generated data: 0.958165373802185\n",
      "Epoch: 2, batch: 0/45,dloss: 0.6342758536338806, gloss: 0.5595607757568359\n",
      "Epoch: 2, batch: 1/45,dloss: 0.6375325918197632, gloss: 0.5853655338287354\n",
      "Epoch: 2, batch: 2/45,dloss: 0.6303783655166626, gloss: 0.5865663290023804\n",
      "Epoch: 2, batch: 3/45,dloss: 0.6050277948379517, gloss: 0.6028738021850586\n",
      "Epoch: 2, batch: 4/45,dloss: 0.6067095398902893, gloss: 0.6007665395736694\n",
      "Epoch: 2, batch: 5/45,dloss: 0.5926584005355835, gloss: 0.6292584538459778\n",
      "Epoch: 2, batch: 6/45,dloss: 0.6315888166427612, gloss: 0.633224606513977\n",
      "Epoch: 2, batch: 7/45,dloss: 0.6109442710876465, gloss: 0.6475783586502075\n",
      "Epoch: 2, batch: 8/45,dloss: 0.6110274791717529, gloss: 0.6695656180381775\n",
      "Epoch: 2, batch: 9/45,dloss: 0.6099727749824524, gloss: 0.6792864203453064\n",
      "Epoch: 2, batch: 10/45,dloss: 0.6042672395706177, gloss: 0.6891303062438965\n",
      "Epoch: 2, batch: 11/45,dloss: 0.6127974987030029, gloss: 0.6842545866966248\n",
      "Epoch: 2, batch: 12/45,dloss: 0.5999355316162109, gloss: 0.6859713792800903\n",
      "Epoch: 2, batch: 13/45,dloss: 0.6065671443939209, gloss: 0.685592532157898\n",
      "Epoch: 2, batch: 14/45,dloss: 0.615234375, gloss: 0.6874864101409912\n",
      "Epoch: 2, batch: 15/45,dloss: 0.6033223271369934, gloss: 0.6872081756591797\n",
      "Epoch: 2, batch: 16/45,dloss: 0.6142935752868652, gloss: 0.7027027606964111\n",
      "Epoch: 2, batch: 17/45,dloss: 0.6066586375236511, gloss: 0.6977582573890686\n",
      "Epoch: 2, batch: 18/45,dloss: 0.6122757196426392, gloss: 0.6871697306632996\n",
      "Epoch: 2, batch: 19/45,dloss: 0.6116671562194824, gloss: 0.6869975924491882\n",
      "Epoch: 2, batch: 20/45,dloss: 0.5924305319786072, gloss: 0.692253053188324\n",
      "Epoch: 2, batch: 21/45,dloss: 0.6089231967926025, gloss: 0.7004495859146118\n",
      "Epoch: 2, batch: 22/45,dloss: 0.6093629598617554, gloss: 0.7147619724273682\n",
      "Epoch: 2, batch: 23/45,dloss: 0.6157506704330444, gloss: 0.7061445713043213\n",
      "Epoch: 2, batch: 24/45,dloss: 0.6201298236846924, gloss: 0.6982083916664124\n",
      "Epoch: 2, batch: 25/45,dloss: 0.6137712001800537, gloss: 0.7216026782989502\n",
      "Epoch: 2, batch: 26/45,dloss: 0.601214587688446, gloss: 0.7170618772506714\n",
      "Epoch: 2, batch: 27/45,dloss: 0.6081718802452087, gloss: 0.7108786106109619\n",
      "Epoch: 2, batch: 28/45,dloss: 0.6187227964401245, gloss: 0.7321031093597412\n",
      "Epoch: 2, batch: 29/45,dloss: 0.6078223586082458, gloss: 0.7344403266906738\n",
      "Epoch: 2, batch: 30/45,dloss: 0.6205558776855469, gloss: 0.7244689464569092\n",
      "Epoch: 2, batch: 31/45,dloss: 0.6275925040245056, gloss: 0.7174705862998962\n",
      "Epoch: 2, batch: 32/45,dloss: 0.6079510450363159, gloss: 0.7485237121582031\n",
      "Epoch: 2, batch: 33/45,dloss: 0.6370992064476013, gloss: 0.7339065074920654\n",
      "Epoch: 2, batch: 34/45,dloss: 0.6024961471557617, gloss: 0.7241345643997192\n",
      "Epoch: 2, batch: 35/45,dloss: 0.5990354418754578, gloss: 0.7276469469070435\n",
      "Epoch: 2, batch: 36/45,dloss: 0.6184091567993164, gloss: 0.7405086755752563\n",
      "Epoch: 2, batch: 37/45,dloss: 0.6313940286636353, gloss: 0.750211775302887\n",
      "Epoch: 2, batch: 38/45,dloss: 0.6034327149391174, gloss: 0.7414384484291077\n",
      "Epoch: 2, batch: 39/45,dloss: 0.6281948089599609, gloss: 0.7743452787399292\n",
      "Epoch: 2, batch: 40/45,dloss: 0.6009944677352905, gloss: 0.7362425327301025\n",
      "Epoch: 2, batch: 41/45,dloss: 0.6174874305725098, gloss: 0.751923680305481\n",
      "Epoch: 2, batch: 42/45,dloss: 0.5969400405883789, gloss: 0.7426931858062744\n",
      "Epoch: 2, batch: 43/45,dloss: 0.5985268354415894, gloss: 0.7511348128318787\n",
      "Epoch: 2, batch: 44/45,dloss: 0.6132210493087769, gloss: 0.7480934858322144\n",
      "Epoch: 2, Accuracy on real data: 0.5911266326904296, Accuracy on generated data: 0.6461027383804321\n",
      "Epoch: 3, batch: 0/45,dloss: 0.6222350001335144, gloss: 0.763667106628418\n",
      "Epoch: 3, batch: 1/45,dloss: 0.6293321847915649, gloss: 0.7734644412994385\n",
      "Epoch: 3, batch: 2/45,dloss: 0.6232790946960449, gloss: 0.7752690315246582\n",
      "Epoch: 3, batch: 3/45,dloss: 0.5874433517456055, gloss: 0.7497053146362305\n",
      "Epoch: 3, batch: 4/45,dloss: 0.6245989799499512, gloss: 0.768740177154541\n",
      "Epoch: 3, batch: 5/45,dloss: 0.6194084882736206, gloss: 0.7628931999206543\n",
      "Epoch: 3, batch: 6/45,dloss: 0.6120282411575317, gloss: 0.7565134763717651\n",
      "Epoch: 3, batch: 7/45,dloss: 0.6372870206832886, gloss: 0.7725275754928589\n",
      "Epoch: 3, batch: 8/45,dloss: 0.6181813478469849, gloss: 0.7724777460098267\n",
      "Epoch: 3, batch: 9/45,dloss: 0.6063472032546997, gloss: 0.8069804310798645\n",
      "Epoch: 3, batch: 10/45,dloss: 0.6154502630233765, gloss: 0.8031686544418335\n",
      "Epoch: 3, batch: 11/45,dloss: 0.6134132742881775, gloss: 0.7699137926101685\n",
      "Epoch: 3, batch: 12/45,dloss: 0.5965734720230103, gloss: 0.7819055318832397\n",
      "Epoch: 3, batch: 13/45,dloss: 0.6093404293060303, gloss: 0.7678438425064087\n",
      "Epoch: 3, batch: 14/45,dloss: 0.6008291840553284, gloss: 0.7743276357650757\n",
      "Epoch: 3, batch: 15/45,dloss: 0.5872031450271606, gloss: 0.7862752079963684\n",
      "Epoch: 3, batch: 16/45,dloss: 0.6059749126434326, gloss: 0.7956593632698059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, batch: 17/45,dloss: 0.588718056678772, gloss: 0.785611629486084\n",
      "Epoch: 3, batch: 18/45,dloss: 0.5911518335342407, gloss: 0.7781992554664612\n",
      "Epoch: 3, batch: 19/45,dloss: 0.5948381423950195, gloss: 0.82975172996521\n",
      "Epoch: 3, batch: 20/45,dloss: 0.567048192024231, gloss: 0.7949709892272949\n",
      "Epoch: 3, batch: 21/45,dloss: 0.5813242197036743, gloss: 0.7879652976989746\n",
      "Epoch: 3, batch: 22/45,dloss: 0.6071549654006958, gloss: 0.7955690622329712\n",
      "Epoch: 3, batch: 23/45,dloss: 0.5795159339904785, gloss: 0.8136553764343262\n",
      "Epoch: 3, batch: 24/45,dloss: 0.5957659482955933, gloss: 0.8213846683502197\n",
      "Epoch: 3, batch: 25/45,dloss: 0.5884613394737244, gloss: 0.7865116000175476\n",
      "Epoch: 3, batch: 26/45,dloss: 0.5807105302810669, gloss: 0.7700284123420715\n",
      "Epoch: 3, batch: 27/45,dloss: 0.5755963325500488, gloss: 0.8022135496139526\n",
      "Epoch: 3, batch: 28/45,dloss: 0.5682511329650879, gloss: 0.8049841523170471\n",
      "Epoch: 3, batch: 29/45,dloss: 0.5879316329956055, gloss: 0.8196221590042114\n",
      "Epoch: 3, batch: 30/45,dloss: 0.5862710475921631, gloss: 0.802770733833313\n",
      "Epoch: 3, batch: 31/45,dloss: 0.5643105506896973, gloss: 0.8057882785797119\n",
      "Epoch: 3, batch: 32/45,dloss: 0.5864517092704773, gloss: 0.8277716636657715\n",
      "Epoch: 3, batch: 33/45,dloss: 0.5804464817047119, gloss: 0.8143705129623413\n",
      "Epoch: 3, batch: 34/45,dloss: 0.5771613121032715, gloss: 0.8017297983169556\n",
      "Epoch: 3, batch: 35/45,dloss: 0.571110725402832, gloss: 0.8019568920135498\n",
      "Epoch: 3, batch: 36/45,dloss: 0.6068174839019775, gloss: 0.7979538440704346\n",
      "Epoch: 3, batch: 37/45,dloss: 0.5538104772567749, gloss: 0.7856229543685913\n",
      "Epoch: 3, batch: 38/45,dloss: 0.5820667743682861, gloss: 0.7837443351745605\n",
      "Epoch: 3, batch: 39/45,dloss: 0.5624510049819946, gloss: 0.7633054256439209\n",
      "Epoch: 3, batch: 40/45,dloss: 0.5783084630966187, gloss: 0.8309605121612549\n",
      "Epoch: 3, batch: 41/45,dloss: 0.6022322773933411, gloss: 0.8244139552116394\n",
      "Epoch: 3, batch: 42/45,dloss: 0.5753688812255859, gloss: 0.8643046617507935\n",
      "Epoch: 3, batch: 43/45,dloss: 0.5616229772567749, gloss: 0.8506188988685608\n",
      "Epoch: 3, batch: 44/45,dloss: 0.5521723031997681, gloss: 0.8486000299453735\n",
      "Epoch: 3, Accuracy on real data: 0.5678147399425506, Accuracy on generated data: 0.5511077761650085\n",
      "Epoch: 4, batch: 0/45,dloss: 0.5902543067932129, gloss: 0.8528121709823608\n",
      "Epoch: 4, batch: 1/45,dloss: 0.5830631256103516, gloss: 0.851259708404541\n",
      "Epoch: 4, batch: 2/45,dloss: 0.599244236946106, gloss: 0.8308984041213989\n",
      "Epoch: 4, batch: 3/45,dloss: 0.5824897885322571, gloss: 0.8336125016212463\n",
      "Epoch: 4, batch: 4/45,dloss: 0.5702345967292786, gloss: 0.8322120904922485\n",
      "Epoch: 4, batch: 5/45,dloss: 0.5885322690010071, gloss: 0.809146523475647\n",
      "Epoch: 4, batch: 6/45,dloss: 0.5703125, gloss: 0.8238909840583801\n",
      "Epoch: 4, batch: 7/45,dloss: 0.6118147373199463, gloss: 0.8230123519897461\n",
      "Epoch: 4, batch: 8/45,dloss: 0.5673409104347229, gloss: 0.8208316564559937\n",
      "Epoch: 4, batch: 9/45,dloss: 0.6178971529006958, gloss: 0.7894719839096069\n",
      "Epoch: 4, batch: 10/45,dloss: 0.6100372076034546, gloss: 0.7675653100013733\n",
      "Epoch: 4, batch: 11/45,dloss: 0.5820897817611694, gloss: 0.7746181488037109\n",
      "Epoch: 4, batch: 12/45,dloss: 0.5729703903198242, gloss: 0.785473644733429\n",
      "Epoch: 4, batch: 13/45,dloss: 0.632702112197876, gloss: 0.7720380425453186\n",
      "Epoch: 4, batch: 14/45,dloss: 0.6210054159164429, gloss: 0.8191999793052673\n",
      "Epoch: 4, batch: 15/45,dloss: 0.6175811290740967, gloss: 0.7967005372047424\n",
      "Epoch: 4, batch: 16/45,dloss: 0.6477229595184326, gloss: 0.7543052434921265\n",
      "Epoch: 4, batch: 17/45,dloss: 0.6396068334579468, gloss: 0.7370141744613647\n",
      "Epoch: 4, batch: 18/45,dloss: 0.6797735095024109, gloss: 0.7509194016456604\n",
      "Epoch: 4, batch: 19/45,dloss: 0.6615656614303589, gloss: 0.7173721790313721\n",
      "Epoch: 4, batch: 20/45,dloss: 0.7126830816268921, gloss: 0.7213035821914673\n",
      "Epoch: 4, batch: 21/45,dloss: 0.6671267151832581, gloss: 0.71293044090271\n",
      "Epoch: 4, batch: 22/45,dloss: 0.673913836479187, gloss: 0.6965651512145996\n",
      "Epoch: 4, batch: 23/45,dloss: 0.7109472155570984, gloss: 0.7021690607070923\n",
      "Epoch: 4, batch: 24/45,dloss: 0.6591305732727051, gloss: 0.7175518870353699\n",
      "Epoch: 4, batch: 25/45,dloss: 0.6633734703063965, gloss: 0.7109770774841309\n",
      "Epoch: 4, batch: 26/45,dloss: 0.6938743591308594, gloss: 0.6949482560157776\n",
      "Epoch: 4, batch: 27/45,dloss: 0.6561216115951538, gloss: 0.7261145710945129\n",
      "Epoch: 4, batch: 28/45,dloss: 0.6913380026817322, gloss: 0.7386825680732727\n",
      "Epoch: 4, batch: 29/45,dloss: 0.6677849888801575, gloss: 0.6974542140960693\n",
      "Epoch: 4, batch: 30/45,dloss: 0.6764506697654724, gloss: 0.7444210052490234\n",
      "Epoch: 4, batch: 31/45,dloss: 0.6829299926757812, gloss: 0.7383813261985779\n",
      "Epoch: 4, batch: 32/45,dloss: 0.6957218647003174, gloss: 0.7319093346595764\n",
      "Epoch: 4, batch: 33/45,dloss: 0.7034953236579895, gloss: 0.7088950276374817\n",
      "Epoch: 4, batch: 34/45,dloss: 0.6689144372940063, gloss: 0.728065550327301\n",
      "Epoch: 4, batch: 35/45,dloss: 0.6805251836776733, gloss: 0.7481649518013\n",
      "Epoch: 4, batch: 36/45,dloss: 0.676813542842865, gloss: 0.7453904151916504\n",
      "Epoch: 4, batch: 37/45,dloss: 0.6832331418991089, gloss: 0.7662297487258911\n",
      "Epoch: 4, batch: 38/45,dloss: 0.6794302463531494, gloss: 0.7644844055175781\n",
      "Epoch: 4, batch: 39/45,dloss: 0.6694384217262268, gloss: 0.7135635614395142\n",
      "Epoch: 4, batch: 40/45,dloss: 0.6792476177215576, gloss: 0.7392399311065674\n",
      "Epoch: 4, batch: 41/45,dloss: 0.6892999410629272, gloss: 0.7362028360366821\n",
      "Epoch: 4, batch: 42/45,dloss: 0.6746532917022705, gloss: 0.773053765296936\n",
      "Epoch: 4, batch: 43/45,dloss: 0.6798499226570129, gloss: 0.755228579044342\n",
      "Epoch: 4, batch: 44/45,dloss: 0.6776974201202393, gloss: 0.7544441819190979\n",
      "Epoch: 4, Accuracy on real data: 0.6985970997810363, Accuracy on generated data: 0.653474600315094\n",
      "Epoch: 5, batch: 0/45,dloss: 0.6892414093017578, gloss: 0.7602901458740234\n",
      "Epoch: 5, batch: 1/45,dloss: 0.6834613680839539, gloss: 0.7223291397094727\n",
      "Epoch: 5, batch: 2/45,dloss: 0.6757737398147583, gloss: 0.7186053395271301\n",
      "Epoch: 5, batch: 3/45,dloss: 0.6999978423118591, gloss: 0.7216648459434509\n",
      "Epoch: 5, batch: 4/45,dloss: 0.684495210647583, gloss: 0.7334378957748413\n",
      "Epoch: 5, batch: 5/45,dloss: 0.7017465829849243, gloss: 0.7301735877990723\n",
      "Epoch: 5, batch: 6/45,dloss: 0.6819965839385986, gloss: 0.7270048260688782\n",
      "Epoch: 5, batch: 7/45,dloss: 0.6781893372535706, gloss: 0.7032485008239746\n",
      "Epoch: 5, batch: 8/45,dloss: 0.714633047580719, gloss: 0.7085742354393005\n",
      "Epoch: 5, batch: 9/45,dloss: 0.6786147952079773, gloss: 0.7167481184005737\n",
      "Epoch: 5, batch: 10/45,dloss: 0.696891188621521, gloss: 0.7118025422096252\n",
      "Epoch: 5, batch: 11/45,dloss: 0.6827689409255981, gloss: 0.7182184457778931\n",
      "Epoch: 5, batch: 12/45,dloss: 0.6961785554885864, gloss: 0.7515023946762085\n",
      "Epoch: 5, batch: 13/45,dloss: 0.6977895498275757, gloss: 0.7556405663490295\n",
      "Epoch: 5, batch: 14/45,dloss: 0.6826322078704834, gloss: 0.7410730123519897\n",
      "Epoch: 5, batch: 15/45,dloss: 0.6705883741378784, gloss: 0.7422727346420288\n",
      "Epoch: 5, batch: 16/45,dloss: 0.6804909110069275, gloss: 0.7295417785644531\n",
      "Epoch: 5, batch: 17/45,dloss: 0.6489700078964233, gloss: 0.7470806837081909\n",
      "Epoch: 5, batch: 18/45,dloss: 0.68474280834198, gloss: 0.7462307214736938\n",
      "Epoch: 5, batch: 19/45,dloss: 0.6556020975112915, gloss: 0.7769818305969238\n",
      "Epoch: 5, batch: 20/45,dloss: 0.6621302366256714, gloss: 0.758460521697998\n",
      "Epoch: 5, batch: 21/45,dloss: 0.6807093620300293, gloss: 0.7536500692367554\n",
      "Epoch: 5, batch: 22/45,dloss: 0.671187162399292, gloss: 0.7940138578414917\n",
      "Epoch: 5, batch: 23/45,dloss: 0.6470761299133301, gloss: 0.7780191898345947\n",
      "Epoch: 5, batch: 24/45,dloss: 0.696785032749176, gloss: 0.7737614512443542\n",
      "Epoch: 5, batch: 25/45,dloss: 0.6528920531272888, gloss: 0.7671693563461304\n",
      "Epoch: 5, batch: 26/45,dloss: 0.6459122896194458, gloss: 0.77520751953125\n",
      "Epoch: 5, batch: 27/45,dloss: 0.6614059209823608, gloss: 0.7634721398353577\n",
      "Epoch: 5, batch: 28/45,dloss: 0.6614919900894165, gloss: 0.7583051919937134\n",
      "Epoch: 5, batch: 29/45,dloss: 0.6789131760597229, gloss: 0.7800817489624023\n",
      "Epoch: 5, batch: 30/45,dloss: 0.6379170417785645, gloss: 0.8027768135070801\n",
      "Epoch: 5, batch: 31/45,dloss: 0.6496247053146362, gloss: 0.7850742340087891\n",
      "Epoch: 5, batch: 32/45,dloss: 0.681675136089325, gloss: 0.8039199113845825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, batch: 33/45,dloss: 0.6579248309135437, gloss: 0.795072078704834\n",
      "Epoch: 5, batch: 34/45,dloss: 0.6475034952163696, gloss: 0.7872962951660156\n",
      "Epoch: 5, batch: 35/45,dloss: 0.6412064433097839, gloss: 0.7727903127670288\n",
      "Epoch: 5, batch: 36/45,dloss: 0.6629592180252075, gloss: 0.7595198154449463\n",
      "Epoch: 5, batch: 37/45,dloss: 0.6523784399032593, gloss: 0.7736433744430542\n",
      "Epoch: 5, batch: 38/45,dloss: 0.6491248607635498, gloss: 0.7646116018295288\n",
      "Epoch: 5, batch: 39/45,dloss: 0.66050785779953, gloss: 0.7613397836685181\n",
      "Epoch: 5, batch: 40/45,dloss: 0.6399863958358765, gloss: 0.7874056696891785\n",
      "Epoch: 5, batch: 41/45,dloss: 0.6383528113365173, gloss: 0.7782950401306152\n",
      "Epoch: 5, batch: 42/45,dloss: 0.6399199366569519, gloss: 0.7853345274925232\n",
      "Epoch: 5, batch: 43/45,dloss: 0.6568453311920166, gloss: 0.7773042917251587\n",
      "Epoch: 5, batch: 44/45,dloss: 0.6437255144119263, gloss: 0.7494907379150391\n",
      "Epoch: 5, Accuracy on real data: 0.6495729470252991, Accuracy on generated data: 0.6315924501419068\n",
      "Epoch: 6, batch: 0/45,dloss: 0.6592499017715454, gloss: 0.7637447714805603\n",
      "Epoch: 6, batch: 1/45,dloss: 0.6716762781143188, gloss: 0.7475695610046387\n",
      "Epoch: 6, batch: 2/45,dloss: 0.6359429359436035, gloss: 0.7466602325439453\n",
      "Epoch: 6, batch: 3/45,dloss: 0.6591013669967651, gloss: 0.7563445568084717\n",
      "Epoch: 6, batch: 4/45,dloss: 0.6537346839904785, gloss: 0.7589566707611084\n",
      "Epoch: 6, batch: 5/45,dloss: 0.6274926662445068, gloss: 0.7581620216369629\n",
      "Epoch: 6, batch: 6/45,dloss: 0.653622031211853, gloss: 0.7415595054626465\n",
      "Epoch: 6, batch: 7/45,dloss: 0.649459719657898, gloss: 0.736956000328064\n",
      "Epoch: 6, batch: 8/45,dloss: 0.6439545154571533, gloss: 0.7908356189727783\n",
      "Epoch: 6, batch: 9/45,dloss: 0.6624974012374878, gloss: 0.7567191123962402\n",
      "Epoch: 6, batch: 10/45,dloss: 0.6438491940498352, gloss: 0.7550682425498962\n",
      "Epoch: 6, batch: 11/45,dloss: 0.6500855684280396, gloss: 0.7449601888656616\n",
      "Epoch: 6, batch: 12/45,dloss: 0.6322771310806274, gloss: 0.7490642666816711\n",
      "Epoch: 6, batch: 13/45,dloss: 0.666103184223175, gloss: 0.7656224966049194\n",
      "Epoch: 6, batch: 14/45,dloss: 0.6601404547691345, gloss: 0.7737945318222046\n",
      "Epoch: 6, batch: 15/45,dloss: 0.6411951780319214, gloss: 0.77406907081604\n",
      "Epoch: 6, batch: 16/45,dloss: 0.6383066177368164, gloss: 0.7901933193206787\n",
      "Epoch: 6, batch: 17/45,dloss: 0.6481164693832397, gloss: 0.776357889175415\n",
      "Epoch: 6, batch: 18/45,dloss: 0.6379566192626953, gloss: 0.7413936853408813\n",
      "Epoch: 6, batch: 19/45,dloss: 0.6534777879714966, gloss: 0.755767822265625\n",
      "Epoch: 6, batch: 20/45,dloss: 0.6708145141601562, gloss: 0.7662691473960876\n",
      "Epoch: 6, batch: 21/45,dloss: 0.6437360048294067, gloss: 0.7853543758392334\n",
      "Epoch: 6, batch: 22/45,dloss: 0.6534765958786011, gloss: 0.8005774021148682\n",
      "Epoch: 6, batch: 23/45,dloss: 0.6524381637573242, gloss: 0.7662707567214966\n",
      "Epoch: 6, batch: 24/45,dloss: 0.634402871131897, gloss: 0.7481768727302551\n",
      "Epoch: 6, batch: 25/45,dloss: 0.6489704847335815, gloss: 0.7894142866134644\n",
      "Epoch: 6, batch: 26/45,dloss: 0.6510240435600281, gloss: 0.7640677690505981\n",
      "Epoch: 6, batch: 27/45,dloss: 0.6393046379089355, gloss: 0.7705956101417542\n",
      "Epoch: 6, batch: 28/45,dloss: 0.6564103364944458, gloss: 0.759240984916687\n",
      "Epoch: 6, batch: 29/45,dloss: 0.6438091993331909, gloss: 0.766157865524292\n",
      "Epoch: 6, batch: 30/45,dloss: 0.648138701915741, gloss: 0.7317467927932739\n",
      "Epoch: 6, batch: 31/45,dloss: 0.656294047832489, gloss: 0.7557252049446106\n",
      "Epoch: 6, batch: 32/45,dloss: 0.6126529574394226, gloss: 0.7566351890563965\n",
      "Epoch: 6, batch: 33/45,dloss: 0.6366967558860779, gloss: 0.7659198045730591\n",
      "Epoch: 6, batch: 34/45,dloss: 0.6406984329223633, gloss: 0.7630223035812378\n",
      "Epoch: 6, batch: 35/45,dloss: 0.635973334312439, gloss: 0.7432690858840942\n",
      "Epoch: 6, batch: 36/45,dloss: 0.6584370136260986, gloss: 0.7814178466796875\n",
      "Epoch: 6, batch: 37/45,dloss: 0.6571115255355835, gloss: 0.7805451154708862\n",
      "Epoch: 6, batch: 38/45,dloss: 0.659536600112915, gloss: 0.7659902572631836\n",
      "Epoch: 6, batch: 39/45,dloss: 0.6484771966934204, gloss: 0.7643237709999084\n",
      "Epoch: 6, batch: 40/45,dloss: 0.6420291662216187, gloss: 0.7905381321907043\n",
      "Epoch: 6, batch: 41/45,dloss: 0.6353787779808044, gloss: 0.8108788132667542\n",
      "Epoch: 6, batch: 42/45,dloss: 0.6617493033409119, gloss: 0.7966940402984619\n",
      "Epoch: 6, batch: 43/45,dloss: 0.6375963687896729, gloss: 0.8015624284744263\n",
      "Epoch: 6, batch: 44/45,dloss: 0.642957329750061, gloss: 0.7851549983024597\n",
      "Epoch: 6, Accuracy on real data: 0.6604207754135132, Accuracy on generated data: 0.6057785749435425\n",
      "Epoch: 7, batch: 0/45,dloss: 0.6520895957946777, gloss: 0.8020750284194946\n",
      "Epoch: 7, batch: 1/45,dloss: 0.6447582840919495, gloss: 0.8004671335220337\n",
      "Epoch: 7, batch: 2/45,dloss: 0.657514214515686, gloss: 0.7881790399551392\n",
      "Epoch: 7, batch: 3/45,dloss: 0.6566711068153381, gloss: 0.7598613500595093\n",
      "Epoch: 7, batch: 4/45,dloss: 0.6523519158363342, gloss: 0.7645360827445984\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a877b8217334>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m           \u001b[0md_model\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m           \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m           latent_dim= latent_dim)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-94e61f3d16ed>\u001b[0m in \u001b[0;36mtrain_gan\u001b[1;34m(gan_model, g_model, d_model, dataset, latent_dim, epochs, batch_size)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0my_gan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_gan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_gan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch: {i + 1}, batch: {j}/{batch_per_epoch},dloss: {d_loss}, gloss: {g_loss}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent_dim = 50\n",
    "dataset = load_mnist_data()\n",
    "g_model = define_generator(latent_dim= latent_dim)\n",
    "d_model = define_discriminator()\n",
    "gan_model = define_gan(g_model= g_model, dis_model= d_model)\n",
    "\n",
    "# Training the GAN for MNIST!!\n",
    "train_gan(gan_model= gan_model,\n",
    "          g_model= g_model,\n",
    "          d_model= d_model,\n",
    "          dataset= dataset,\n",
    "          latent_dim= latent_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
