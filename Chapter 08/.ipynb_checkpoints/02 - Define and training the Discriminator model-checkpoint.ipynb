{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(input_shape= (32,32,3)):\n",
    "    model = keras.models.Sequential()\n",
    "    # Input is a 32*32*3 image\n",
    "    model.add(keras.layers.Conv2D(filters= 64,\n",
    "                                  kernel_size= (3,3),\n",
    "                                  padding= 'same',\n",
    "                                  input_shape= input_shape))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(keras.layers.Conv2D(filters= 64,\n",
    "                                  kernel_size= (3,3),\n",
    "                                  strides= (2,2),\n",
    "                                  padding= 'same'))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    # Input is a 16*16*3 image\n",
    "    model.add(keras.layers.Conv2D(filters= 128,\n",
    "                                  kernel_size= (4,4),\n",
    "                                  strides= (2,2),\n",
    "                                  padding= 'same'))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    # Input is a 8*8*3 image\n",
    "    model.add(keras.layers.Conv2D(filters= 256,\n",
    "                                  kernel_size= (4,4),\n",
    "                                  strides= (2,2),\n",
    "                                  padding= 'same'))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    \n",
    "    # Input is now 4*4*3\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(0.4))\n",
    "    model.add(keras.layers.Dense(units= 1,\n",
    "                                 activation= 'sigmoid'))\n",
    "    opt = keras.optimizers.adam(learning_rate= 0.0002, beta_1= 0.5)\n",
    "    model.compile(loss= 'binary_crossentropy', optimizer= opt, metrics= ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_sample(n_samples):\n",
    "    X = np.random.rand(32 * 32 * 3 * n_samples)\n",
    "    X = X.reshape((n_samples, 32, 32, 3))\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_sample(n_samples):\n",
    "    cifar = keras.datasets.cifar10.load_data()\n",
    "    (X_train, _), (_, _) = cifar\n",
    "    X_train = (X_train.astype('float32') - 127.5) / 127.5\n",
    "    ix = np.random.randint(0, len(X_train), n_samples)\n",
    "    X = X_train[ix, :, :, :]\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dis(d_model, iters= 1, batch_size= 256):\n",
    "    half_batch = int(batch_size / 2)\n",
    "    (dataset, _) , (_, _) = keras.datasets.cifar10.load_data()\n",
    "    batch_per_epoch = int(len(dataset)/batch_size)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        for j in range(batch_per_epoch):\n",
    "            X_real, y_real = generate_real_sample(half_batch)\n",
    "            X_fake, y_fake = generate_fake_sample(half_batch)\n",
    "            \n",
    "            d_model.train_on_batch(X_real, y_real)\n",
    "            d_model.train_on_batch(X_fake, y_fake)\n",
    "            print(f'{i}, {j}/{batch_per_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0/195\n",
      "0, 1/195\n",
      "0, 2/195\n",
      "0, 3/195\n",
      "0, 4/195\n",
      "0, 5/195\n",
      "0, 6/195\n",
      "0, 7/195\n",
      "0, 8/195\n",
      "0, 9/195\n",
      "0, 10/195\n",
      "0, 11/195\n",
      "0, 12/195\n",
      "0, 13/195\n",
      "0, 14/195\n",
      "0, 15/195\n",
      "0, 16/195\n",
      "0, 17/195\n",
      "0, 18/195\n",
      "0, 19/195\n",
      "0, 20/195\n",
      "0, 21/195\n",
      "0, 22/195\n",
      "0, 23/195\n",
      "0, 24/195\n",
      "0, 25/195\n",
      "0, 26/195\n",
      "0, 27/195\n",
      "0, 28/195\n",
      "0, 29/195\n",
      "0, 30/195\n",
      "0, 31/195\n",
      "0, 32/195\n",
      "0, 33/195\n",
      "0, 34/195\n",
      "0, 35/195\n",
      "0, 36/195\n",
      "0, 37/195\n",
      "0, 38/195\n",
      "0, 39/195\n",
      "0, 40/195\n",
      "0, 41/195\n",
      "0, 42/195\n",
      "0, 43/195\n",
      "0, 44/195\n",
      "0, 45/195\n",
      "0, 46/195\n",
      "0, 47/195\n",
      "0, 48/195\n",
      "0, 49/195\n",
      "0, 50/195\n",
      "0, 51/195\n",
      "0, 52/195\n",
      "0, 53/195\n",
      "0, 54/195\n",
      "0, 55/195\n",
      "0, 56/195\n",
      "0, 57/195\n",
      "0, 58/195\n",
      "0, 59/195\n",
      "0, 60/195\n",
      "0, 61/195\n",
      "0, 62/195\n",
      "0, 63/195\n",
      "0, 64/195\n",
      "0, 65/195\n",
      "0, 66/195\n",
      "0, 67/195\n",
      "0, 68/195\n",
      "0, 69/195\n",
      "0, 70/195\n",
      "0, 71/195\n",
      "0, 72/195\n",
      "0, 73/195\n",
      "0, 74/195\n",
      "0, 75/195\n",
      "0, 76/195\n",
      "0, 77/195\n",
      "0, 78/195\n",
      "0, 79/195\n",
      "0, 80/195\n",
      "0, 81/195\n",
      "0, 82/195\n",
      "0, 83/195\n",
      "0, 84/195\n",
      "0, 85/195\n",
      "0, 86/195\n",
      "0, 87/195\n",
      "0, 88/195\n",
      "0, 89/195\n",
      "0, 90/195\n",
      "0, 91/195\n",
      "0, 92/195\n",
      "0, 93/195\n",
      "0, 94/195\n",
      "0, 95/195\n",
      "0, 96/195\n",
      "0, 97/195\n",
      "0, 98/195\n",
      "0, 99/195\n",
      "0, 100/195\n",
      "0, 101/195\n",
      "0, 102/195\n",
      "0, 103/195\n",
      "0, 104/195\n",
      "0, 105/195\n",
      "0, 106/195\n",
      "0, 107/195\n",
      "0, 108/195\n",
      "0, 109/195\n",
      "0, 110/195\n",
      "0, 111/195\n",
      "0, 112/195\n",
      "0, 113/195\n",
      "0, 114/195\n",
      "0, 115/195\n",
      "0, 116/195\n",
      "0, 117/195\n",
      "0, 118/195\n",
      "0, 119/195\n",
      "0, 120/195\n",
      "0, 121/195\n",
      "0, 122/195\n",
      "0, 123/195\n",
      "0, 124/195\n",
      "0, 125/195\n",
      "0, 126/195\n",
      "0, 127/195\n",
      "0, 128/195\n",
      "0, 129/195\n",
      "0, 130/195\n",
      "0, 131/195\n",
      "0, 132/195\n",
      "0, 133/195\n",
      "0, 134/195\n",
      "0, 135/195\n",
      "0, 136/195\n",
      "0, 137/195\n",
      "0, 138/195\n",
      "0, 139/195\n",
      "0, 140/195\n",
      "0, 141/195\n",
      "0, 142/195\n",
      "0, 143/195\n",
      "0, 144/195\n",
      "0, 145/195\n",
      "0, 146/195\n",
      "0, 147/195\n",
      "0, 148/195\n",
      "0, 149/195\n",
      "0, 150/195\n",
      "0, 151/195\n",
      "0, 152/195\n",
      "0, 153/195\n",
      "0, 154/195\n",
      "0, 155/195\n",
      "0, 156/195\n",
      "0, 157/195\n",
      "0, 158/195\n",
      "0, 159/195\n",
      "0, 160/195\n",
      "0, 161/195\n",
      "0, 162/195\n",
      "0, 163/195\n",
      "0, 164/195\n",
      "0, 165/195\n",
      "0, 166/195\n",
      "0, 167/195\n",
      "0, 168/195\n",
      "0, 169/195\n",
      "0, 170/195\n",
      "0, 171/195\n",
      "0, 172/195\n",
      "0, 173/195\n",
      "0, 174/195\n",
      "0, 175/195\n",
      "0, 176/195\n",
      "0, 177/195\n",
      "0, 178/195\n",
      "0, 179/195\n",
      "0, 180/195\n",
      "0, 181/195\n",
      "0, 182/195\n",
      "0, 183/195\n",
      "0, 184/195\n",
      "0, 185/195\n",
      "0, 186/195\n",
      "0, 187/195\n",
      "0, 188/195\n",
      "0, 189/195\n",
      "0, 190/195\n",
      "0, 191/195\n",
      "0, 192/195\n",
      "0, 193/195\n",
      "0, 194/195\n"
     ]
    }
   ],
   "source": [
    "model = define_discriminator()\n",
    "train_dis(d_model= model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 3ms/step\n",
      "[0.002123761281836778, 1.0]\n",
      "128/128 [==============================] - 0s 2ms/step\n",
      "[0.0009418386616744101, 1.0]\n"
     ]
    }
   ],
   "source": [
    "X_real, y_real = generate_real_sample(128)\n",
    "X_fake, y_fake = generate_fake_sample(128)\n",
    "print(model.evaluate(X_real, y_real))\n",
    "print(model.evaluate(X_fake, y_fake))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
